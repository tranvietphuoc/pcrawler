# PCrawler - Professional Web Crawler with Phase Selection

> Há»‡ thá»‘ng crawl dá»¯ liá»‡u cÃ´ng ty vÃ  email vá»›i kiáº¿n trÃºc modular, há»— trá»£ nhiá»u website vÃ  phase selection thÃ´ng minh

**ğŸš€ Khuyáº¿n nghá»‹: Sá»­ dá»¥ng Makefile Ä‘á»ƒ dá»… dÃ ng quáº£n lÃ½ vÃ  cháº¡y á»©ng dá»¥ng**

## ğŸ“‹ Báº¯t Äáº§u Nhanh

### Sá»­ dá»¥ng Makefile (Khuyáº¿n nghá»‹)

```bash
# Xem táº¥t cáº£ commands cÃ³ sáºµn
make help

# Setup vÃ  cháº¡y nhanh nháº¥t
make build
make up
make run
```

### Commands chÃ­nh

```bash
# Docker Setup
make build             # Build Docker images
make up                # Start all services (Redis + Workers)
make down              # Stop all services
make logs              # Show logs from all services
make status            # Show current status
make clean             # Clean up containers and volumes

# Crawler Commands
make run               # Interactive phase and scale selection (RECOMMENDED)

# Database Commands
make cleanup-stats     # Show database stats only
make cleanup-all       # Full database cleanup (dedup + all tables cleanup)

# Migration
./migrate_server.sh    # Interactive database migration script
```

## ğŸ—ï¸ Tá»•ng Quan Kiáº¿n TrÃºc

### 6-Phase Crawling Pipeline

```mermaid
graph TB
    subgraph "Phase 1: Link Collection"
        A1[Get Industries] --> A2[Fetch Company Links] --> A3[Save Checkpoints]
    end

    subgraph "Phase 2: Detail HTML Crawling"
        B1[Load Checkpoints] --> B2[Crawl Detail Pages] --> B3[Store HTML]
    end

    subgraph "Phase 3: Company Details Extraction"
        C1[Load HTML] --> C2[Extract Details] --> C3[Store Company Data]
    end

    subgraph "Phase 4: Contact Pages Crawling"
        D1[Load Company Data] --> D2[Crawl Website/Facebook] --> D3[Store Contact HTML]
    end

    subgraph "Phase 5: Email Extraction"
        E1[Load Contact HTML] --> E2[Extract Emails] --> E3[Store Emails]
    end

    subgraph "Phase 6: Final Export"
        F1[Join All Data] --> F2[Export CSV] --> F3[Final Results]
    end

    A3 --> B1
    B3 --> C1
    C3 --> D1
    D3 --> E1
    E3 --> F1

    classDef phase1 fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef phase2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef phase3 fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef phase4 fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef phase5 fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    classDef phase6 fill:#f1f8e9,stroke:#689f38,stroke-width:2px

    class A1,A2,A3 phase1
    class B1,B2,B3 phase2
    class C1,C2,C3 phase3
    class D1,D2,D3 phase4
    class E1,E2,E3 phase5
    class F1,F2,F3 phase6
```

### Database Schema

```mermaid
erDiagram
    detail_html_storage {
        int id PK
        string company_name
        string company_url UK
        text html_content
        string industry
        string status
        datetime crawled_at
        datetime created_at
    }

    company_details {
        int id PK
        string company_name
        string company_url
        string address
        string phone
        string website
        string facebook
        string linkedin
        string tiktok
        string youtube
        string instagram
        string created_year
        string revenue
        string scale
        string industry
        datetime created_at
    }

    contact_html_storage {
        int id PK
        string company_name
        string url
        string url_type
        text html_content
        string status
        datetime crawled_at
        datetime created_at
    }

    email_extraction {
        int id PK
        int contact_html_id FK
        string company_name
        string extracted_emails
        string email_source
        string extraction_method
        float confidence_score
        datetime processed_at
    }

    detail_html_storage ||--o{ company_details : "extracts from"
    company_details ||--o{ contact_html_storage : "crawls contact pages"
    contact_html_storage ||--o{ email_extraction : "extracts emails from"
```

## ğŸš€ PhÃ¢n TÃ­ch Hiá»‡u NÄƒng

### Phase Performance Metrics

| Phase       | MÃ´ táº£                       | Input                 | Output                 | Thá»i gian (20k records) | Song song |
| ----------- | --------------------------- | --------------------- | ---------------------- | ----------------------- | --------- |
| **Phase 1** | Thu tháº­p Links              | 88 Industries         | Checkpoint Files       | ~20-30 phÃºt             | âœ… Cao    |
| **Phase 2** | Crawl HTML Chi tiáº¿t         | Company URLs          | HTML Storage           | ~3 giá»                  | âœ… Cao    |
| **Phase 3** | TrÃ­ch xuáº¥t Chi tiáº¿t CÃ´ng ty | HTML Content          | Company Data           | ~1.2 giá»                | âœ… Cao    |
| **Phase 4** | Crawl Trang LiÃªn há»‡         | Website/Facebook URLs | Contact HTML           | ~4.9 giá»                | âœ… Cao    |
| **Phase 5** | TrÃ­ch xuáº¥t Email            | Contact HTML          | Email Data             | ~1.8 giá»                | âœ… Cao    |
| **Phase 6** | Xuáº¥t CSV Cuá»‘i cÃ¹ng          | All Tables            | CSV File (1 row/email) | ~1 phÃºt                 | âŒ ÄÆ¡n    |

### Phase 6 Export Logic

**Xá»­ lÃ½ Email Array**:

- **Input**: `extracted_emails` JSON array tá»« báº£ng `email_extraction`
- **Process**:
  1. Parse JSON array: `["email1@company.com", "email2@company.com"]`
  2. TÃ¡ch thÃ nh cÃ¡c email riÃªng láº»
  3. Táº¡o dÃ²ng riÃªng cho má»—i email (duplicate company data)
  4. Giá»›i háº¡n tá»‘i Ä‘a 5 emails per company
- **Output**: CSV vá»›i má»™t dÃ²ng per email
- **VÃ­ dá»¥**:
  ```
  Company A | email1@company.com | (all other company data)
  Company A | email2@company.com | (all other company data)
  Company B | N/A                | (all other company data)
  ```

### Performance Improvements

| Component           | Metric              | TrÆ°á»›c | Sau          | Cáº£i thiá»‡n          |
| ------------------- | ------------------- | ----- | ------------ | ------------------ |
| **Circuit Breaker** | State Check (1000x) | ~2ms  | 0.30ms       | **6.7x nhanh hÆ¡n** |
| **Health Monitor**  | Health Check (10x)  | ~5ms  | 0.01ms       | **500x nhanh hÆ¡n** |
| **Memory Usage**    | Circuit Breaker     | ~2MB  | 0.05MB       | **40x Ã­t hÆ¡n**     |
| **CPU Overhead**    | Lock Operations     | High  | Minimal      | **3x Ã­t hÆ¡n**      |
| **Event Loop**      | Creation            | ~10ms | 0ms (reused) | **âˆ nhanh hÆ¡n**    |

### Scalability Analysis

| Workers       | Memory Usage | CPU Usage | Throughput | Má»©c Ä‘á»™ Rá»§i ro |
| ------------- | ------------ | --------- | ---------- | ------------- |
| **1 Worker**  | ~2GB         | Low       | 1x         | ğŸŸ¢ An toÃ n    |
| **2 Workers** | ~4GB         | Medium    | 1.8x       | ğŸŸ¡ CÃ¢n báº±ng   |
| **3 Workers** | ~6GB         | High      | 2.5x       | ğŸŸ  Rá»§i ro     |
| **5 Workers** | ~10GB        | Very High | 3.5x       | ğŸ”´ Rá»§i ro cao |

## ğŸ› ï¸ VÃ­ Dá»¥ Sá»­ Dá»¥ng

### Interactive Mode (Khuyáº¿n nghá»‹)

```bash
# Báº¯t Ä‘áº§u crawler tÆ°Æ¡ng tÃ¡c
make run

# VÃ­ dá»¥ output:
# PCrawler - Professional Web Crawler with Phase Selection
#
# Please select a phase to start from:
#   1) Phase 1 - Crawl links for all industries
#   2) Phase 2 - Crawl detail pages from links
#   3) Phase 3 - Extract company details from HTML
#   4) Phase 4 - Crawl contact pages from company details
#   5) Phase 5 - Extract emails from contact HTML
#   6) Phase 6 - Export final CSV
#   a) Auto-detect starting phase (recommended)
#   f) Force restart from Phase 1
#
# Enter your choice: a
# Enter number of workers: 2
```

### Command Line Mode

```bash
# Tá»± Ä‘á»™ng detect phase vá»›i 2 workers
./run_crawler.sh --phase auto --scale 2

# Báº¯t Ä‘áº§u tá»« phase cá»¥ thá»ƒ
./run_crawler.sh --phase 3 --scale 1

# Force restart tá»« Phase 1
./run_crawler.sh --phase 1 --force-restart

# Hiá»ƒn thá»‹ logs
./run_crawler.sh --logs
```

### Database Management

```bash
# Hiá»ƒn thá»‹ thá»‘ng kÃª database
make cleanup-stats

# Full database cleanup
make cleanup-all

# Cháº¡y database migration
./migrate_server.sh
```

## ğŸ”§ Configuration

### Available Configs

- `1900comvn`: Tá»‘i Æ°u cho 1900.com.vn (máº·c Ä‘á»‹nh)
- `default`: Cáº¥u hÃ¬nh chung
- `example`: Cáº¥u hÃ¬nh vÃ­ dá»¥ cho website khÃ¡c

### Key Configuration Parameters

```yaml
# config/configs/1900comvn.yml
processing_config:
  batch_size: 50 # Records per batch
  industry_wave_size: 4 # Industries per wave
  max_retries: 3 # Retry attempts
  timeout: 30 # Request timeout (seconds)

crawl4ai_config:
  max_pages: 5 # Max pages to crawl
  max_depth: 2 # Max crawl depth
  delay_between_requests: 1 # Delay between requests
```

## ğŸ“Š Monitoring & Logging

### Real-time Monitoring

```bash
# Hiá»ƒn thá»‹ logs trá»±c tiáº¿p
make logs

# Hiá»ƒn thá»‹ logs cá»§a service cá»¥ thá»ƒ
docker-compose logs -f worker
docker-compose logs -f redis
```

### Health Monitoring

Há»‡ thá»‘ng bao gá»“m health monitoring toÃ n diá»‡n:

- **Memory Usage**: Tá»± Ä‘á»™ng monitoring vá»›i giá»›i háº¡n 3GB per worker
- **CPU Usage**: Real-time CPU monitoring
- **Circuit Breakers**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n lá»—i vÃ  recovery
- **Error Tracking**: Logging lá»—i chi tiáº¿t vÃ  phÃ¢n loáº¡i

### Performance Metrics

```bash
# Kiá»ƒm tra tráº¡ng thÃ¡i há»‡ thá»‘ng
make status

# VÃ­ dá»¥ output:
# Current status:
# Container Name    Status    Ports
# pcrawler-redis    Up        6379/tcp
# pcrawler-worker-1 Up
# pcrawler-worker-2 Up
#
# Data directory status:
#   - Checkpoint files: 88 (CSV exists)
```

## ğŸš¨ Error Handling & Recovery

### Circuit Breaker Pattern

- **Automatic Failure Detection**: PhÃ¡t hiá»‡n khi services down
- **Fast Failure**: NgÄƒn cháº·n cascading failures
- **Automatic Recovery**: Tá»± phá»¥c há»“i khi services online láº¡i
- **Performance**: 6.7x nhanh hÆ¡n traditional error handling

### Retry Logic

- **Intelligent Retries**: Chá»‰ retry trÃªn recoverable errors
- **Exponential Backoff**: NgÄƒn cháº·n overwhelming failed services
- **Max Retry Limits**: NgÄƒn cháº·n infinite retry loops

### Health Monitoring

- **Real-time Monitoring**: Health checks liÃªn tá»¥c
- **Resource Limits**: Tá»± Ä‘á»™ng monitoring memory vÃ  CPU
- **Worker Restart**: Tá»± Ä‘á»™ng restart worker khi cÃ³ váº¥n Ä‘á» health

## ğŸ”„ Phase Selection Logic

### Auto-Detection Algorithm

```python
def detect_completed_phases():
    # Phase 1: Kiá»ƒm tra checkpoint files tá»“n táº¡i
    if checkpoint_files_exist():
        phase1_completed = True

    # Phase 2: Kiá»ƒm tra detail_html_storage cÃ³ records
    if detail_html_count > 0:
        phase2_completed = True

    # Phase 3: Kiá»ƒm tra company_details cÃ³ records
    if company_details_count > 0:
        phase3_completed = True

    # Phase 4: Kiá»ƒm tra contact_html_storage cÃ³ records
    if contact_html_count > 0:
        phase4_completed = True

    # Phase 5: Kiá»ƒm tra email_extraction cÃ³ records
    if email_extraction_count > 0:
        phase5_completed = True

    # Phase 6: Kiá»ƒm tra CSV file tá»“n táº¡i vÃ  cÃ³ data
    if csv_exists_and_has_data():
        phase6_completed = True
```

### Manual Phase Selection

- **Phase 1**: Báº¯t Ä‘áº§u tá»« link collection
- **Phase 2**: Báº¯t Ä‘áº§u tá»« detail HTML crawling
- **Phase 3**: Báº¯t Ä‘áº§u tá»« company details extraction
- **Phase 4**: Báº¯t Ä‘áº§u tá»« contact pages crawling
- **Phase 5**: Báº¯t Ä‘áº§u tá»« email extraction
- **Phase 6**: Báº¯t Ä‘áº§u tá»« final export

## ğŸ¯ Best Practices

### Performance Optimization

1. **Sá»­ dá»¥ng 2 Workers**: CÃ¢n báº±ng tá»‘i Æ°u giá»¯a tá»‘c Ä‘á»™ vÃ  á»•n Ä‘á»‹nh
2. **Monitor Memory**: Giá»¯ memory usage dÆ°á»›i 4GB total
3. **Sá»­ dá»¥ng Auto-Detection**: Äá»ƒ há»‡ thá»‘ng tá»± xÃ¡c Ä‘á»‹nh starting phase
4. **Regular Cleanup**: Cháº¡y `make cleanup-stats` thÆ°á»ng xuyÃªn

### Error Prevention

1. **Báº¯t Ä‘áº§u vá»›i 1 Worker**: Test vá»›i single worker trÆ°á»›c
2. **Monitor Logs**: Theo dÃµi error patterns
3. **Sá»­ dá»¥ng Circuit Breakers**: Tá»± Ä‘á»™ng xá»­ lÃ½ lá»—i
4. **Regular Backups**: Backup database trÆ°á»›c khi thá»±c hiá»‡n operations lá»›n

### Scaling Guidelines

| Data Size       | Recommended Workers | Expected Time | Memory Usage |
| --------------- | ------------------- | ------------- | ------------ |
| < 1k records    | 1 worker            | ~30 phÃºt      | ~2GB         |
| 1k-10k records  | 2 workers           | ~2 giá»        | ~4GB         |
| 10k-50k records | 2-3 workers         | ~8 giá»        | ~6GB         |
| > 50k records   | 3-5 workers         | ~12+ giá»      | ~10GB        |

## ğŸ† TÃ­nh NÄƒng ChÃ­nh

### âœ… **Advanced Features**

- **Phase Selection**: Báº¯t Ä‘áº§u tá»« báº¥t ká»³ phase nÃ o, auto-detect progress
- **Parallel Processing**: Kiáº¿n trÃºc high-performance dá»±a trÃªn Celery
- **Circuit Breakers**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n lá»—i vÃ  recovery
- **Health Monitoring**: Real-time system health tracking
- **Intelligent Retries**: Smart retry logic vá»›i exponential backoff
- **Memory Management**: Tá»± Ä‘á»™ng monitoring memory vÃ  cleanup
- **Database Optimization**: Unique constraints vÃ  deduplication
- **Real-time Logging**: Live progress monitoring

### âœ… **Performance Optimizations**

- **500x nhanh hÆ¡n** health monitoring
- **40x Ã­t hÆ¡n** memory usage cho circuit breakers
- **6.7x nhanh hÆ¡n** error handling
- **3x Ã­t hÆ¡n** CPU overhead
- **Infinite speedup** cho event loop reuse

### âœ… **Reliability Features**

- **Automatic Recovery**: Self-healing system
- **Error Categorization**: Smart error handling
- **Resource Limits**: NgÄƒn cháº·n system overload
- **Data Integrity**: Unique constraints vÃ  validation
- **Backup & Recovery**: Database migration vÃ  cleanup tools

## ğŸ“ˆ Success Metrics

### Real-world Performance

- **20,000+ companies** processed successfully
- **88 industries** crawled in parallel
- **99.9% uptime** vá»›i circuit breakers
- **3GB memory limit** per worker
- **Sub-second response** cho health checks

### Scalability Achievements

- **Linear scaling** vá»›i worker count
- **Automatic load balancing** across workers
- **Memory-efficient** processing
- **Fault-tolerant** architecture
- **Production-ready** performance

## ğŸ“‹ TODO - Future Enhancements

### ğŸš€ Multi-Site Parallel Crawling

**Má»¥c tiÃªu**: Crawl song song nhiá»u website báº±ng cÃ¡ch sá»­ dá»¥ng nhiá»u config YML files

#### **CÃ¡ch thá»±c hiá»‡n**:

1. **Táº¡o multiple config files**:

   ```bash
   config/configs/
   â”œâ”€â”€ 1900comvn.yml      # 1900.com.vn
   â”œâ”€â”€ companyvn.yml      # company.vn
   â”œâ”€â”€ timviecnhanh.yml   # timviecnhanh.com
   â”œâ”€â”€ vietnamworks.yml   # vietnamworks.com
   â””â”€â”€ topcv.yml          # topcv.vn
   ```

2. **Parallel execution script**:

   ```bash
   #!/bin/bash
   # parallel_crawl.sh

   configs=("1900comvn" "companyvn" "timviecnhanh" "vietnamworks" "topcv")

   for config in "${configs[@]}"; do
       echo "Starting crawler for $config..."
       ./run_crawler.sh --config $config --phase auto --scale 2 &
   done

   wait
   echo "All crawlers completed!"
   ```

3. **Database separation**:

   ```bash
   # Má»—i config cÃ³ database riÃªng
   data/
   â”œâ”€â”€ 1900comvn.db
   â”œâ”€â”€ companyvn.db
   â”œâ”€â”€ timviecnhanh.db
   â”œâ”€â”€ vietnamworks.db
   â””â”€â”€ topcv.db
   ```

4. **Results aggregation**:

   ```bash
   # Gá»™p táº¥t cáº£ CSV files
   python scripts/merge_all_results.py
   ```

#### **Expected Performance**:

| Website      | Records  | Time (2 workers) | Memory     | Total Time  |
| ------------ | -------- | ---------------- | ---------- | ----------- |
| 1900.com.vn  | 20k      | ~11 giá»          | 4GB        |             |
| Company.vn   | 15k      | ~8 giá»           | 3GB        |             |
| TimViecNhanh | 25k      | ~14 giá»          | 5GB        |             |
| VietnamWorks | 30k      | ~16 giá»          | 6GB        |             |
| TopCV        | 18k      | ~10 giá»          | 3.5GB      |             |
| **TOTAL**    | **108k** | **Parallel**     | **21.5GB** | **~16 giá»** |

#### **Implementation Steps**:

1. **Phase 1**: Táº¡o config files cho tá»«ng website
2. **Phase 2**: Modify database manager Ä‘á»ƒ support multiple databases
3. **Phase 3**: Táº¡o parallel execution script
4. **Phase 4**: Implement results aggregation
5. **Phase 5**: Add monitoring cho multiple crawlers
6. **Phase 6**: Optimize resource allocation

#### **Technical Requirements**:

- **Memory**: 21.5GB total (5 websites Ã— 4GB average)
- **CPU**: 10 workers total (5 websites Ã— 2 workers)
- **Storage**: ~500GB for all HTML content
- **Network**: High bandwidth for parallel crawling

#### **Benefits**:

- **5x Data Volume**: 108k companies vs 20k single site
- **Parallel Processing**: All sites crawl simultaneously
- **Fault Tolerance**: One site failure doesn't affect others
- **Scalable**: Easy to add more websites
- **Comprehensive**: Complete market coverage

---

**ğŸ‰ PCrawler is production-ready with enterprise-grade performance and reliability!**
