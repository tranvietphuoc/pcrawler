# PCrawler - Modular Web Crawler System

> H·ªá th·ªëng crawl d·ªØ li·ªáu c√¥ng ty v√† email v·ªõi ki·∫øn tr√∫c modular, h·ªó tr·ª£ nhi·ªÅu website

**üöÄ Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng Makefile ƒë·ªÉ d·ªÖ d√†ng qu·∫£n l√Ω v√† ch·∫°y ·ª©ng d·ª•ng**

## Quick Start

### S·ª≠ d·ª•ng Makefile (Khuy·∫øn ngh·ªã)

```bash
# Xem t·∫•t c·∫£ commands c√≥ s·∫µn
make help

# Setup v√† ch·∫°y nhanh nh·∫•t
make docker-build
make docker-scale-2
# Ho·∫∑c
make docker-crawl
```

### Commands ch√≠nh

```bash
# Docker Setup
make docker-build    # Build Docker images
make docker-up       # Start services (Redis + Worker)
make docker-down     # Stop all services
make docker-logs     # Show logs

# Crawling
make crawl           # Start crawling (local)
make docker-crawl    # Start crawling (Docker)

# Scaling (t·ªëi ∆∞u performance)
make docker-scale-1  # Safe mode (1 worker) - Low risk, slower
make docker-scale-2  # Fast mode (2 workers) - Balanced speed/risk

# Manual
make docker-merge    # Merge CSV files
```

## Architecture Flow

### Crawling Process Flow

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor': '#ff0000', 'primaryTextColor': '#000000', 'primaryBorderColor': '#000000', 'lineColor': '#000000', 'secondaryColor': '#ffffff', 'tertiaryColor': '#ffffff', 'background': '#ffffff', 'mainBkg': '#ffffff', 'secondBkg': '#ffffff', 'tertiaryBkg': '#ffffff'}}}%%
graph TB
    subgraph "Row 1: Data Collection"
        A["üîó Phase 0<br/>Link Fetching"] --> B["üìÑ Phase 1<br/>Detail Crawling"]
        B --> C["üîç Phase 2<br/>Extract Details"]
    end

    subgraph "Row 2: Contact & Email"
        D["üåê Phase 3<br/>Contact Crawling"] --> E["üìß Phase 4<br/>Email Extraction"]
        E --> F["üìä Phase 5<br/>Final Export"]
    end

    C --> D

    %% Styling
    classDef phase0 fill:#e1f5fe,stroke:#01579b,stroke-width:4px,font-size:18px,font-weight:bold
    classDef phase1 fill:#f3e5f5,stroke:#4a148c,stroke-width:4px,font-size:18px,font-weight:bold
    classDef phase2 fill:#e8f5e8,stroke:#1b5e20,stroke-width:4px,font-size:18px,font-weight:bold
    classDef phase3 fill:#fff3e0,stroke:#e65100,stroke-width:4px,font-size:18px,font-weight:bold
    classDef phase4 fill:#fce4ec,stroke:#880e4f,stroke-width:4px,font-size:18px,font-weight:bold
    classDef phase5 fill:#f1f8e9,stroke:#33691e,stroke-width:4px,font-size:18px,font-weight:bold

    class A phase0
    class B phase1
    class C phase2
    class D phase3
    class E phase4
    class F phase5
```

### Database Tables

```mermaid
erDiagram
    detail_html_storage {
        int id PK
        string company_name
        string company_url
        text html_content
        string industry
        datetime created_at
    }

    company_details {
        int id PK
        string company_name
        string company_url
        string address
        string phone
        string website
        string facebook
        string linkedin
        string tiktok
        string youtube
        string instagram
        string created_year
        string revenue
        string scale
        string industry
        datetime created_at
    }

    contact_html_storage {
        int id PK
        string company_name
        string url
        string url_type
        text html_content
        datetime created_at
    }

    email_extraction {
        int id PK
        string company_name
        string extracted_email
        string email_source
        float confidence_score
        datetime created_at
    }

    detail_html_storage ||--o{ company_details : "extracts from"
    company_details ||--o{ contact_html_storage : "crawls contact pages"
    contact_html_storage ||--o{ email_extraction : "extracts emails from"
```

### Phase Details

#### **Phase 0: Link Fetching (PARALLEL)**

- **Input**: Base URL, Industries list
- **Process**: Submit industry link fetching tasks to Celery workers
- **Output**: All company links collected
- **Time**: ~20-30 minutes (vs 3+ hours sequential)

#### **Phase 1: Detail HTML Crawling (PARALLEL)**

- **Input**: Company links from Phase 0
- **Process**: Crawl detail pages, store HTML content
- **Output**: HTML stored in `detail_html_storage` table
- **Time**: ~3 hours for 22k companies

#### **Phase 2: Company Details Extraction (PARALLEL)**

- **Input**: HTML from `detail_html_storage`
- **Process**: Extract company info (name, address, phone, website, social media)
- **Output**: Structured data in `company_details` table
- **Time**: ~1.2 hours for 22k companies

#### **Phase 3: Contact HTML Crawling (PARALLEL)**

- **Input**: Website/Facebook URLs from `company_details`
- **Process**: Crawl contact pages, store HTML content
- **Output**: HTML stored in `contact_html_storage` table
- **Time**: ~4.9 hours for 22k companies

#### **Phase 4: Email Extraction (PARALLEL)**

- **Input**: HTML from `contact_html_storage`
- **Process**: Extract emails using Crawl4AI queries
- **Output**: Emails in `email_extraction` table
- **Time**: ~1.8 hours for 22k companies

#### **Phase 5: Final CSV Export**

- **Input**: All tables (detail_html_storage, company_details, email_extraction)
- **Process**: Join tables, create final CSV
- **Output**: `company_contacts.csv` with all data
- **Time**: ~1 minute

### Performance Comparison

| Phase              | Sequential    | Parallel      | Improvement       |
| ------------------ | ------------- | ------------- | ----------------- |
| Link Fetching      | 3.3 hours     | 20 minutes    | **10x faster**    |
| Detail Crawling    | 3 hours       | 3 hours       | Same              |
| Details Extraction | 1.2 hours     | 1.2 hours     | Same              |
| Contact Crawling   | 4.9 hours     | 4.9 hours     | Same              |
| Email Extraction   | 1.8 hours     | 1.8 hours     | Same              |
| CSV Export         | 1 minute      | 1 minute      | Same              |
| **TOTAL**          | **~14 hours** | **~11 hours** | **3 hours saved** |

### Celery Tasks

```mermaid
graph TB
    subgraph "Main Process"
        A[Main Orchestrator]
    end

    subgraph "Celery Tasks"
        B[fetch_industry_links]
        C[crawl_detail_pages]
        D[extract_company_details]
        E[crawl_contact_pages_from_details]
        F[extract_emails_from_contact]
        G[export_final_csv]
    end

    subgraph "Celery Workers"
        H[Worker 1]
        I[Worker 2]
        J[Worker N]
    end

    subgraph "Database Storage"
        K[detail_html_storage]
        L[company_details]
        M[contact_html_storage]
        N[email_extraction]
    end

    A --> B
    A --> C
    A --> D
    A --> E
    A --> F
    A --> G

    B --> H
    C --> I
    D --> J
    E --> H
    F --> I
    G --> J

    H --> K
    I --> L
    J --> M
    H --> N

    %% Styling
    classDef main fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef task fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef worker fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef storage fill:#fff3e0,stroke:#f57c00,stroke-width:2px

    class A main
    class B,C,D,E,F,G task
    class H,I,J worker
    class K,L,M,N storage
```

#### Task Descriptions

| Task                               | Purpose                             | Input                 | Output                    |
| ---------------------------------- | ----------------------------------- | --------------------- | ------------------------- |
| `fetch_industry_links`             | Get company links for each industry | Industry ID, Name     | List of company URLs      |
| `crawl_detail_pages`               | Crawl company detail pages          | Company URLs          | HTML stored in DB         |
| `extract_company_details`          | Extract company info from HTML      | HTML content          | Structured company data   |
| `crawl_contact_pages_from_details` | Crawl contact pages                 | Website/Facebook URLs | Contact HTML stored in DB |
| `extract_emails_from_contact`      | Extract emails from contact HTML    | Contact HTML          | Email addresses           |
| `export_final_csv`                 | Create final CSV file               | All database tables   | Final CSV output          |

## Workflow

### 1. Setup Docker

```bash
make docker-build    # Build images
make docker-scale-2  # Start v·ªõi 2 workers (t·ªëi ∆∞u)
```

### 2. Start Crawling

```bash
make docker-crawl    # B·∫Øt ƒë·∫ßu crawl
make docker-logs     # Xem logs real-time
```

### 3. Monitor Progress

```bash
make docker-logs     # Xem logs
# Ho·∫∑c
docker-compose logs -f worker
```

### 4. Merge Results (n·∫øu c·∫ßn)

```bash
make docker-merge    # G·ªôp t·∫•t c·∫£ CSV files
```

## Performance Tips

### Scaling Options:

- **Safe mode (1 worker)**: √çt r·ªßi ro, ch·∫≠m h∆°n
- **Fast mode (2 workers)**: C√¢n b·∫±ng t·ªëc ƒë·ªô/r·ªßi ro (khuy·∫øn ngh·ªã)

### Memory Management:

- T·ª± ƒë·ªông gi·ªõi h·∫°n RAM 3GB/worker
- Garbage collection sau m·ªói task
- Worker restart ƒë·ªãnh k·ª≥ ƒë·ªÉ tr√°nh memory leak

## Ki·∫øn tr√∫c

### Modules ch√≠nh:

- `app/crawler/list_crawler.py`: Crawl danh s√°ch ng√†nh v√† link c√¥ng ty
- `app/crawler/detail_crawler.py`: Crawl chi ti·∫øt c√¥ng ty (song song)
- `app/extractor/email_extractor.py`: Extract email b·∫±ng crawl4ai
- `app/tasks/`: Celery tasks cho x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô
- `app/utils/batching_writer.py`: Ghi CSV an to√†n theo batch
- `config/`: H·ªá th·ªëng config YAML linh ho·∫°t

### Workflow:

1. **Crawl Industries** ‚Üí L·∫•y danh s√°ch ng√†nh
2. **Crawl Company Links** ‚Üí L·∫•y link c√¥ng ty theo ng√†nh
3. **Create Tasks** ‚Üí Chia th√†nh batch v√† g·ª≠i v√†o Celery queue
4. **Process Tasks** ‚Üí M·ªói task crawl chi ti·∫øt + extract email
5. **Merge Results** ‚Üí G·ªôp t·∫•t c·∫£ file CSV cu·ªëi c√πng

## Configuration

### Config c√≥ s·∫µn:

- `default`: C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
- `1900comvn`: C·∫•u h√¨nh cho 1900.com.vn
- `example`: C·∫•u h√¨nh v√≠ d·ª• cho website kh√°c

### T·∫°o config m·ªõi:

```bash
# Copy config c√≥ s·∫µn
cp config/configs/default.yml config/configs/mywebsite.yml

# Ch·ªânh s·ª≠a file config
vim config/configs/mywebsite.yml

# Validate config
uv run python -m app.main validate --config mywebsite

# Ch·∫°y v·ªõi config m·ªõi
uv run python -m app.main crawl --config mywebsite
```

### C·∫•u tr√∫c YAML:

```yaml
website:
  name: "Website Name"
  base_url: "https://example.com"

xpath:
  company_name: "//h1[@class='company-title']"
  company_address: "//div[@class='address']"
  # ... c√°c xpath kh√°c

crawl4ai:
  website_query: "Extract business emails from website"
  facebook_query: "Extract business emails from Facebook"

processing:
  batch_size: 30
  write_batch_size: 150
  max_concurrent_pages: 6
  max_retries: 2
  delay_range: [1.5, 3.0]
  timeout: 60000
  network_timeout: 20000
  stealth_mode: true

output:
  output_dir: "data/tasks"
  final_output: "data/companies.csv"

fieldnames:
  - industry_name
  # ... c√°c field kh√°c
```

## D·ªØ li·ªáu Output

### Fields trong CSV:

- `industry_name`: Th√¥ng tin ng√†nh
- `name`, `address`, `website`, `phone`: Th√¥ng tin c∆° b·∫£n c√¥ng ty
- `created_year`, `revenue`, `scale`: Th√¥ng tin kinh doanh
- `link`, `facebook`, `linkedin`, `tiktok`, `youtube`, `instagram`: Social media
- `extracted_emails`: Email ƒë∆∞·ª£c extract (ph√¢n c√°ch b·∫±ng "; ")
- `email_source`: Ngu·ªìn email (Facebook/Website/N/A)

### Phone number format:

- T·ª± ƒë·ªông clean v√† format th√†nh d·∫°ng: `+84933802408`
- Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, d·∫•u c√°ch
- Chuy·ªÉn ƒë·ªïi t·ª´ 0 th√†nh +84 (VD: 0933802408 ‚Üí +84933802408)
- Gi·ªØ nguy√™n n·∫øu ƒë√£ c√≥ +84
- ƒê·∫£m b·∫£o ƒë·ªô d√†i 11-12 s·ªë (bao g·ªìm 84)

## Development

### Setup Development Environment:

```bash
# C√†i ƒë·∫∑t dependencies
source ./.venv/bin/activate.sh
pip install -r requirements.txt

# Ho·∫∑c v·ªõi uv
uv pip install -r requirements.txt
```

### Project Structure:

```
pcrawler/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ crawler/          # Crawling modules
‚îÇ   ‚îú‚îÄ‚îÄ extractor/        # Email extraction
‚îÇ   ‚îú‚îÄ‚îÄ tasks/           # Celery tasks
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ main.py          # Main orchestrator
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ configs/         # YAML configurations
‚îÇ   ‚îî‚îÄ‚îÄ crawler_config.py # Config loader
‚îú‚îÄ‚îÄ tests/               # Test files
‚îú‚îÄ‚îÄ data/               # Output data
‚îú‚îÄ‚îÄ docker-compose.yml  # Docker setup
‚îú‚îÄ‚îÄ Makefile           # Development commands
‚îî‚îÄ‚îÄ pyproject.toml     # Project metadata
```

## Docker

### Services:

- `redis`: Message broker cho Celery
- `worker`: Celery worker x·ª≠ l√Ω tasks (c√≥ th·ªÉ scale)
- `app`: Main application

### Memory Limits:

- **Worker**: 3GB RAM limit, 2GB reservation
- **Max tasks per child**: 20 tasks (t·ª± ƒë·ªông restart)
- **Max memory per child**: 2GB (t·ª± ƒë·ªông restart n·∫øu v∆∞·ª£t)

### Environment Variables:

```bash
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
CELERY_WORKER_MAX_TASKS_PER_CHILD=20
CELERY_WORKER_MAX_MEMORY_PER_CHILD=2000000
```

## Monitoring & Logging

### Memory Monitoring:

```bash
# Xem logs v·ªõi memory info
make docker-logs

# Memory logs s·∫Ω hi·ªÉn th·ªã:
# [MEMORY][Task xxx] start: 150 MB
# [MEMORY][Task xxx] after GC: 120 MB (freed ~30 MB)
# [MEMORY][Batch] before: 200 MB
# [MEMORY][Batch] after GC: 180 MB (freed ~20 MB)
```

### Log Levels:

- `DEBUG`: Chi ti·∫øt nh·∫•t, d√†nh cho development
- `INFO`: Th√¥ng tin chung (m·∫∑c ƒë·ªãnh)
- `WARNING`: C·∫£nh b√°o
- `ERROR`: L·ªói

### Docker Logs:

```bash
# Xem t·∫•t c·∫£ logs
make docker-logs

# Xem logs worker
docker-compose logs -f worker

# Xem logs app
docker-compose logs -f app
```

## Error Handling

### Retry Logic:

- T·ª± ƒë·ªông retry khi crawl th·∫•t b·∫°i
- Configurable retry count v√† delay
- Graceful handling c·ªßa network errors

### Task Recovery:

- M·ªói task t·∫°o file ri√™ng
- C√≥ th·ªÉ g·ªôp l·∫°i file n·∫øu qu√° tr√¨nh b·ªã gi√°n ƒëo·∫°n
- Kh√¥ng m·∫•t d·ªØ li·ªáu khi task fail

## Security & Best Practices

### Rate Limiting:

- Configurable delay gi·ªØa c√°c request
- Respect robots.txt
- User-Agent rotation

### Data Validation:

- Validate config tr∆∞·ªõc khi ch·∫°y
- Clean v√† format d·ªØ li·ªáu
- Validate email format

## Contributing

1. Fork project
2. T·∫°o feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. T·∫°o Pull Request

### Code Style:

```bash
# Format code (n·∫øu c√≥ black)
uv run black .

# Check linting (n·∫øu c√≥ flake8)
uv run flake8 .

# Run tests (n·∫øu c√≥ pytest)
uv run pytest
```

## License

MIT License - xem file [LICENSE](LICENSE) ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.

## Troubleshooting

### Common Issues:

1. **Docker kh√¥ng start:**

   ```bash
   make docker-down
   make docker-build
   make docker-scale-2
   ```

2. **Celery worker kh√¥ng nh·∫≠n tasks:**

   ```bash
   # Check Redis connection
   docker-compose exec redis redis-cli ping

   # Restart worker
   docker-compose restart worker
   ```

3. **Memory usage cao:**

   ```bash
   # Xem memory logs
   make docker-logs | grep MEMORY

   # Worker s·∫Ω t·ª± restart sau 20 tasks ho·∫∑c 2GB RAM
   # Kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng
   ```

4. **Crawl ch·∫≠m:**

   ```bash
   # TƒÉng s·ªë workers
   make docker-scale-2  # Thay v√¨ docker-scale-1

   # Ho·∫∑c scale cao h∆°n (c·∫©n th·∫≠n)
   docker-compose up --scale worker=4 -d
   ```

### Debug Mode:

```bash
# Xem logs chi ti·∫øt
make docker-logs

# Xem logs worker
docker-compose logs -f worker --tail=100

# Xem logs app
docker-compose logs -f app --tail=100
```

## Support

- Email: phuoctv.ut@gmail.com
- Issues: [GitHub Issues](https://github.com/tranvietphuoc/pcrawler/issues)
- Documentation: [Wiki](https://github.com/tranvietphuoc/pcrawler/wiki)
