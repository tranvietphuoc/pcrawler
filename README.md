# PCrawler - Modular Web Crawler System

> H·ªá th·ªëng crawl d·ªØ li·ªáu c√¥ng ty v√† email v·ªõi ki·∫øn tr√∫c modular, h·ªó tr·ª£ nhi·ªÅu website

**üöÄ Khuy·∫øn ngh·ªã: S·ª≠ d·ª•ng Makefile ƒë·ªÉ d·ªÖ d√†ng qu·∫£n l√Ω v√† ch·∫°y ·ª©ng d·ª•ng**

## Quick Start

### S·ª≠ d·ª•ng Makefile (Khuy·∫øn ngh·ªã)

```bash
# Xem t·∫•t c·∫£ commands c√≥ s·∫µn
make help

# Setup v√† ch·∫°y nhanh nh·∫•t
make docker-build
make docker-scale-2
# Ho·∫∑c
make docker-crawl
```

### Commands ch√≠nh

```bash
# Docker Setup
make docker-build    # Build Docker images
make docker-up       # Start services (Redis + Worker)
make docker-down     # Stop all services
make docker-logs     # Show logs

# Crawling
make crawl           # Start crawling (local)
make docker-crawl    # Start crawling (Docker)

# Scaling (t·ªëi ∆∞u performance)
make docker-scale-1  # Safe mode (1 worker) - Low risk, slower
make docker-scale-2  # Fast mode (2 workers) - Balanced speed/risk

# Manual
make docker-merge    # Merge CSV files
```

## Workflow

### 1. Setup Docker

```bash
make docker-build    # Build images
make docker-scale-2  # Start v·ªõi 2 workers (t·ªëi ∆∞u)
```

### 2. Start Crawling

```bash
make docker-crawl    # B·∫Øt ƒë·∫ßu crawl
make docker-logs     # Xem logs real-time
```

### 3. Monitor Progress

```bash
make docker-logs     # Xem logs
# Ho·∫∑c
docker-compose logs -f worker
```

### 4. Merge Results (n·∫øu c·∫ßn)

```bash
make docker-merge    # G·ªôp t·∫•t c·∫£ CSV files
```

## Performance Tips

### Scaling Options:

- **Safe mode (1 worker)**: √çt r·ªßi ro, ch·∫≠m h∆°n
- **Fast mode (2 workers)**: C√¢n b·∫±ng t·ªëc ƒë·ªô/r·ªßi ro (khuy·∫øn ngh·ªã)

### Memory Management:

- T·ª± ƒë·ªông gi·ªõi h·∫°n RAM 3GB/worker
- Garbage collection sau m·ªói task
- Worker restart ƒë·ªãnh k·ª≥ ƒë·ªÉ tr√°nh memory leak

## Ki·∫øn tr√∫c

### Modules ch√≠nh:

- `app/crawler/list_crawler.py`: Crawl danh s√°ch ng√†nh v√† link c√¥ng ty
- `app/crawler/detail_crawler.py`: Crawl chi ti·∫øt c√¥ng ty (song song)
- `app/extractor/email_extractor.py`: Extract email b·∫±ng crawl4ai
- `app/tasks/`: Celery tasks cho x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô
- `app/utils/batching_writer.py`: Ghi CSV an to√†n theo batch
- `config/`: H·ªá th·ªëng config YAML linh ho·∫°t

### Workflow:

1. **Crawl Industries** ‚Üí L·∫•y danh s√°ch ng√†nh
2. **Crawl Company Links** ‚Üí L·∫•y link c√¥ng ty theo ng√†nh
3. **Create Tasks** ‚Üí Chia th√†nh batch v√† g·ª≠i v√†o Celery queue
4. **Process Tasks** ‚Üí M·ªói task crawl chi ti·∫øt + extract email
5. **Merge Results** ‚Üí G·ªôp t·∫•t c·∫£ file CSV cu·ªëi c√πng

## Configuration

### Config c√≥ s·∫µn:

- `default`: C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
- `1900comvn`: C·∫•u h√¨nh cho 1900.com.vn
- `example`: C·∫•u h√¨nh v√≠ d·ª• cho website kh√°c

### T·∫°o config m·ªõi:

```bash
# Copy config c√≥ s·∫µn
cp config/configs/default.yml config/configs/mywebsite.yml

# Ch·ªânh s·ª≠a file config
vim config/configs/mywebsite.yml

# Validate config
uv run python -m app.main validate --config mywebsite

# Ch·∫°y v·ªõi config m·ªõi
uv run python -m app.main crawl --config mywebsite
```

### C·∫•u tr√∫c YAML:

```yaml
website:
  name: "Website Name"
  base_url: "https://example.com"

xpath:
  company_name: "//h1[@class='company-title']"
  company_address: "//div[@class='address']"
  # ... c√°c xpath kh√°c

crawl4ai:
  website_query: "Extract business emails from website"
  facebook_query: "Extract business emails from Facebook"

processing:
  batch_size: 20
  max_concurrent_pages: 5
  write_batch_size: 100

output:
  output_dir: "data/tasks"
  final_output: "data/companies.csv"

fieldnames:
  - industry_name
  # ... c√°c field kh√°c
```

## D·ªØ li·ªáu Output

### Fields trong CSV:

- `industry_name`: Th√¥ng tin ng√†nh
- `name`, `address`, `website`, `phone`: Th√¥ng tin c∆° b·∫£n c√¥ng ty
- `created_year`, `revenue`, `scale`: Th√¥ng tin kinh doanh
- `link`, `facebook`, `linkedin`, `tiktok`, `youtube`, `instagram`: Social media
- `extracted_emails`: Email ƒë∆∞·ª£c extract (ph√¢n c√°ch b·∫±ng "; ")
- `email_source`: Ngu·ªìn email (Facebook/Website/N/A)

### Phone number format:

- T·ª± ƒë·ªông clean v√† format th√†nh d·∫°ng: `+84933802408`
- Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát, d·∫•u c√°ch
- Chuy·ªÉn ƒë·ªïi t·ª´ 0 th√†nh +84 (VD: 0933802408 ‚Üí +84933802408)
- Gi·ªØ nguy√™n n·∫øu ƒë√£ c√≥ +84
- ƒê·∫£m b·∫£o ƒë·ªô d√†i 11-12 s·ªë (bao g·ªìm 84)

## Development

### Setup Development Environment:

```bash
# C√†i ƒë·∫∑t dependencies
source ./.venv/bin/activate.sh
pip install -r requirements.txt

# Ho·∫∑c v·ªõi uv
uv pip install -r requirements.txt
```

### Project Structure:

```
pcrawler/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ crawler/          # Crawling modules
‚îÇ   ‚îú‚îÄ‚îÄ extractor/        # Email extraction
‚îÇ   ‚îú‚îÄ‚îÄ tasks/           # Celery tasks
‚îÇ   ‚îú‚îÄ‚îÄ utils/           # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ main.py          # Main orchestrator
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ configs/         # YAML configurations
‚îÇ   ‚îî‚îÄ‚îÄ crawler_config.py # Config loader
‚îú‚îÄ‚îÄ tests/               # Test files
‚îú‚îÄ‚îÄ data/               # Output data
‚îú‚îÄ‚îÄ docker-compose.yml  # Docker setup
‚îú‚îÄ‚îÄ Makefile           # Development commands
‚îî‚îÄ‚îÄ pyproject.toml     # Project metadata
```

## Docker

### Services:

- `redis`: Message broker cho Celery
- `worker`: Celery worker x·ª≠ l√Ω tasks (c√≥ th·ªÉ scale)
- `app`: Main application

### Memory Limits:

- **Worker**: 3GB RAM limit, 2GB reservation
- **Max tasks per child**: 20 tasks (t·ª± ƒë·ªông restart)
- **Max memory per child**: 2GB (t·ª± ƒë·ªông restart n·∫øu v∆∞·ª£t)

### Environment Variables:

```bash
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
CELERY_WORKER_MAX_TASKS_PER_CHILD=20
CELERY_WORKER_MAX_MEMORY_PER_CHILD=2000000
```

## Monitoring & Logging

### Memory Monitoring:

```bash
# Xem logs v·ªõi memory info
make docker-logs

# Memory logs s·∫Ω hi·ªÉn th·ªã:
# [MEMORY][Task xxx] start: 150 MB
# [MEMORY][Task xxx] after GC: 120 MB (freed ~30 MB)
# [MEMORY][Batch] before: 200 MB
# [MEMORY][Batch] after GC: 180 MB (freed ~20 MB)
```

### Log Levels:

- `DEBUG`: Chi ti·∫øt nh·∫•t, d√†nh cho development
- `INFO`: Th√¥ng tin chung (m·∫∑c ƒë·ªãnh)
- `WARNING`: C·∫£nh b√°o
- `ERROR`: L·ªói

### Docker Logs:

```bash
# Xem t·∫•t c·∫£ logs
make docker-logs

# Xem logs worker
docker-compose logs -f worker

# Xem logs app
docker-compose logs -f app
```

## Error Handling

### Retry Logic:

- T·ª± ƒë·ªông retry khi crawl th·∫•t b·∫°i
- Configurable retry count v√† delay
- Graceful handling c·ªßa network errors

### Task Recovery:

- M·ªói task t·∫°o file ri√™ng
- C√≥ th·ªÉ g·ªôp l·∫°i file n·∫øu qu√° tr√¨nh b·ªã gi√°n ƒëo·∫°n
- Kh√¥ng m·∫•t d·ªØ li·ªáu khi task fail

## Security & Best Practices

### Rate Limiting:

- Configurable delay gi·ªØa c√°c request
- Respect robots.txt
- User-Agent rotation

### Data Validation:

- Validate config tr∆∞·ªõc khi ch·∫°y
- Clean v√† format d·ªØ li·ªáu
- Validate email format

## Contributing

1. Fork project
2. T·∫°o feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. T·∫°o Pull Request

### Code Style:

```bash
# Format code (n·∫øu c√≥ black)
uv run black .

# Check linting (n·∫øu c√≥ flake8)
uv run flake8 .

# Run tests (n·∫øu c√≥ pytest)
uv run pytest
```

## License

MIT License - xem file [LICENSE](LICENSE) ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.

## Troubleshooting

### Common Issues:

1. **Docker kh√¥ng start:**

   ```bash
   make docker-down
   make docker-build
   make docker-scale-2
   ```

2. **Celery worker kh√¥ng nh·∫≠n tasks:**

   ```bash
   # Check Redis connection
   docker-compose exec redis redis-cli ping

   # Restart worker
   docker-compose restart worker
   ```

3. **Memory usage cao:**

   ```bash
   # Xem memory logs
   make docker-logs | grep MEMORY

   # Worker s·∫Ω t·ª± restart sau 20 tasks ho·∫∑c 2GB RAM
   # Kh√¥ng c·∫ßn can thi·ªáp th·ªß c√¥ng
   ```

4. **Crawl ch·∫≠m:**

   ```bash
   # TƒÉng s·ªë workers
   make docker-scale-2  # Thay v√¨ docker-scale-1

   # Ho·∫∑c scale cao h∆°n (c·∫©n th·∫≠n)
   docker-compose up --scale worker=4 -d
   ```

### Debug Mode:

```bash
# Xem logs chi ti·∫øt
make docker-logs

# Xem logs worker
docker-compose logs -f worker --tail=100

# Xem logs app
docker-compose logs -f app --tail=100
```

## Support

- Email: phuoctv.ut@gmail.com
- Issues: [GitHub Issues](https://github.com/tranvietphuoc/pcrawler/issues)
- Documentation: [Wiki](https://github.com/tranvietphuoc/pcrawler/wiki)
