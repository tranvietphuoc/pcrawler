# PCrawler - Modular Web Crawler System

> Há»‡ thá»‘ng crawl dá»¯ liá»‡u cÃ´ng ty vÃ  email vá»›i kiáº¿n trÃºc modular, há»— trá»£ nhiá»u website

**ğŸš€ Khuyáº¿n nghá»‹: Sá»­ dá»¥ng Makefile Ä‘á»ƒ dá»… dÃ ng quáº£n lÃ½ vÃ  cháº¡y á»©ng dá»¥ng**

## Quick Start

### Sá»­ dá»¥ng Makefile (Khuyáº¿n nghá»‹)

```bash
# Xem táº¥t cáº£ commands cÃ³ sáºµn
make help

# Setup vÃ  cháº¡y nhanh nháº¥t
make docker-build
make docker-scale-2
# Hoáº·c
make docker-crawl
```

### Commands chÃ­nh

```bash
# Docker Setup
make docker-build    # Build Docker images
make docker-up       # Start services (Redis + Worker)
make docker-down     # Stop all services
make docker-logs     # Show logs

# Crawling
make crawl           # Start crawling (local)
make docker-crawl    # Start crawling (Docker)

# Scaling (tá»‘i Æ°u performance)
make docker-scale-1  # Safe mode (1 worker) - Low risk, slower
make docker-scale-2  # Fast mode (2 workers) - Balanced speed/risk

# Manual
make docker-merge    # Merge CSV files
```

## Architecture Flow

### Complete Crawling Process

```mermaid
%%{init: {'theme':'base', 'themeVariables': { 'primaryColor': '#ff0000', 'primaryTextColor': '#000000', 'primaryBorderColor': '#000000', 'lineColor': '#000000', 'secondaryColor': '#ffffff', 'tertiaryColor': '#ffffff', 'background': '#ffffff', 'mainBkg': '#ffffff', 'secondBkg': '#ffffff', 'tertiaryBkg': '#ffffff'}}}%%
graph LR
    subgraph "ğŸ”— Phase 0: Link Fetching"
        A["ğŸš€ Start<br/>Crawling Process"] --> B["ğŸ“‹ Get Industries<br/>List from Website"]
        B --> C["ğŸ“¤ Submit Link Tasks<br/>to Celery Workers"]
        C --> D["âš¡ Parallel Link Fetching<br/>Multiple Workers"]
        D --> E["ğŸ“Š Collect Links<br/>All Company URLs"]
    end

    subgraph "ğŸ“„ Phase 1: Detail Crawling"
        E --> F["ğŸ“¤ Submit Detail Tasks<br/>to Celery Workers"]
        F --> G["âš¡ Parallel Detail Crawling<br/>Multiple Workers"]
        G --> H["ğŸ’¾ Store HTML in DB<br/>detail_html_storage"]
    end

    subgraph "ğŸ” Phase 2: Extract Details"
        H --> I["ğŸ“¤ Submit Extract Tasks<br/>to Celery Workers"]
        I --> J["ğŸ“¥ Load HTML from DB<br/>detail_html_storage"]
        J --> K["ğŸ” Extract Company Info<br/>Name, Address, Phone, etc."]
        K --> L["ğŸ’¾ Store in company_details<br/>Structured Data"]
    end

    subgraph "ğŸŒ Phase 3: Contact Crawling"
        L --> M["ğŸ“¤ Submit Contact Tasks<br/>to Celery Workers"]
        M --> N["ğŸ“¥ Load Website/Facebook URLs<br/>from company_details"]
        N --> O["âš¡ Parallel Contact Crawling<br/>Multiple Workers"]
        O --> P["ğŸ’¾ Store Contact HTML<br/>contact_html_storage"]
    end

    subgraph "ğŸ“§ Phase 4: Email Extraction"
        P --> Q["ğŸ“¤ Submit Email Tasks<br/>to Celery Workers"]
        Q --> R["ğŸ“¥ Load Contact HTML<br/>contact_html_storage"]
        R --> S["ğŸ“§ Extract Emails<br/>using Crawl4AI"]
        S --> T["ğŸ’¾ Store Emails<br/>email_extraction"]
    end

    subgraph "ğŸ“Š Phase 5: Final Export"
        T --> U["ğŸ“¤ Submit Export Task<br/>to Celery Worker"]
        U --> V["ğŸ”— Join All Tables<br/>Combine Data"]
        V --> W["ğŸ“„ Export CSV<br/>Final Result"]
        W --> X["âœ… End<br/>Process Complete"]
    end

    %% Styling with larger fonts and boxes
    classDef phase0 fill:#e1f5fe,stroke:#01579b,stroke-width:4px,font-size:14px,font-weight:bold
    classDef phase1 fill:#f3e5f5,stroke:#4a148c,stroke-width:4px,font-size:14px,font-weight:bold
    classDef phase2 fill:#e8f5e8,stroke:#1b5e20,stroke-width:4px,font-size:14px,font-weight:bold
    classDef phase3 fill:#fff3e0,stroke:#e65100,stroke-width:4px,font-size:14px,font-weight:bold
    classDef phase4 fill:#fce4ec,stroke:#880e4f,stroke-width:4px,font-size:14px,font-weight:bold
    classDef phase5 fill:#f1f8e9,stroke:#33691e,stroke-width:4px,font-size:14px,font-weight:bold

    class A,B,C,D,E phase0
    class F,G,H phase1
    class I,J,K,L phase2
    class M,N,O,P phase3
    class Q,R,S,T phase4
    class U,V,W,X phase5
```

### Database Tables

```mermaid
erDiagram
    detail_html_storage {
        int id PK
        string company_name
        string company_url
        text html_content
        string industry
        datetime created_at
    }

    company_details {
        int id PK
        string company_name
        string company_url
        string address
        string phone
        string website
        string facebook
        string linkedin
        string tiktok
        string youtube
        string instagram
        string created_year
        string revenue
        string scale
        string industry
        datetime created_at
    }

    contact_html_storage {
        int id PK
        string company_name
        string url
        string url_type
        text html_content
        datetime created_at
    }

    email_extraction {
        int id PK
        string company_name
        string extracted_email
        string email_source
        float confidence_score
        datetime created_at
    }

    detail_html_storage ||--o{ company_details : "extracts from"
    company_details ||--o{ contact_html_storage : "crawls contact pages"
    contact_html_storage ||--o{ email_extraction : "extracts emails from"
```

### Phase Details

#### **Phase 0: Link Fetching (PARALLEL)**

- **Input**: Base URL, Industries list
- **Process**: Submit industry link fetching tasks to Celery workers
- **Output**: All company links collected
- **Time**: ~20-30 minutes (vs 3+ hours sequential)

#### **Phase 1: Detail HTML Crawling (PARALLEL)**

- **Input**: Company links from Phase 0
- **Process**: Crawl detail pages, store HTML content
- **Output**: HTML stored in `detail_html_storage` table
- **Time**: ~3 hours for 22k companies

#### **Phase 2: Company Details Extraction (PARALLEL)**

- **Input**: HTML from `detail_html_storage`
- **Process**: Extract company info (name, address, phone, website, social media)
- **Output**: Structured data in `company_details` table
- **Time**: ~1.2 hours for 22k companies

#### **Phase 3: Contact HTML Crawling (PARALLEL)**

- **Input**: Website/Facebook URLs from `company_details`
- **Process**: Crawl contact pages, store HTML content
- **Output**: HTML stored in `contact_html_storage` table
- **Time**: ~4.9 hours for 22k companies

#### **Phase 4: Email Extraction (PARALLEL)**

- **Input**: HTML from `contact_html_storage`
- **Process**: Extract emails using Crawl4AI queries
- **Output**: Emails in `email_extraction` table
- **Time**: ~1.8 hours for 22k companies

#### **Phase 5: Final CSV Export**

- **Input**: All tables (detail_html_storage, company_details, email_extraction)
- **Process**: Join tables, create final CSV
- **Output**: `company_contacts.csv` with all data
- **Time**: ~1 minute

### Performance Comparison

| Phase              | Sequential    | Parallel      | Improvement       |
| ------------------ | ------------- | ------------- | ----------------- |
| Link Fetching      | 3.3 hours     | 20 minutes    | **10x faster**    |
| Detail Crawling    | 3 hours       | 3 hours       | Same              |
| Details Extraction | 1.2 hours     | 1.2 hours     | Same              |
| Contact Crawling   | 4.9 hours     | 4.9 hours     | Same              |
| Email Extraction   | 1.8 hours     | 1.8 hours     | Same              |
| CSV Export         | 1 minute      | 1 minute      | Same              |
| **TOTAL**          | **~14 hours** | **~11 hours** | **3 hours saved** |

### Celery Tasks

```mermaid
graph TB
    subgraph "Main Process"
        A[Main Orchestrator]
    end

    subgraph "Celery Tasks"
        B[fetch_industry_links]
        C[crawl_detail_pages]
        D[extract_company_details]
        E[crawl_contact_pages_from_details]
        F[extract_emails_from_contact]
        G[export_final_csv]
    end

    subgraph "Celery Workers"
        H[Worker 1]
        I[Worker 2]
        J[Worker N]
    end

    subgraph "Database Storage"
        K[detail_html_storage]
        L[company_details]
        M[contact_html_storage]
        N[email_extraction]
    end

    A --> B
    A --> C
    A --> D
    A --> E
    A --> F
    A --> G

    B --> H
    C --> I
    D --> J
    E --> H
    F --> I
    G --> J

    H --> K
    I --> L
    J --> M
    H --> N

    %% Styling
    classDef main fill:#e3f2fd,stroke:#1976d2,stroke-width:3px
    classDef task fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef worker fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    classDef storage fill:#fff3e0,stroke:#f57c00,stroke-width:2px

    class A main
    class B,C,D,E,F,G task
    class H,I,J worker
    class K,L,M,N storage
```

#### Task Descriptions

| Task                               | Purpose                             | Input                 | Output                    |
| ---------------------------------- | ----------------------------------- | --------------------- | ------------------------- |
| `fetch_industry_links`             | Get company links for each industry | Industry ID, Name     | List of company URLs      |
| `crawl_detail_pages`               | Crawl company detail pages          | Company URLs          | HTML stored in DB         |
| `extract_company_details`          | Extract company info from HTML      | HTML content          | Structured company data   |
| `crawl_contact_pages_from_details` | Crawl contact pages                 | Website/Facebook URLs | Contact HTML stored in DB |
| `extract_emails_from_contact`      | Extract emails from contact HTML    | Contact HTML          | Email addresses           |
| `export_final_csv`                 | Create final CSV file               | All database tables   | Final CSV output          |

## Workflow

### 1. Setup Docker

```bash
make docker-build    # Build images
make docker-scale-2  # Start vá»›i 2 workers (tá»‘i Æ°u)
```

### 2. Start Crawling

```bash
make docker-crawl    # Báº¯t Ä‘áº§u crawl
make docker-logs     # Xem logs real-time
```

### 3. Monitor Progress

```bash
make docker-logs     # Xem logs
# Hoáº·c
docker-compose logs -f worker
```

### 4. Merge Results (náº¿u cáº§n)

```bash
make docker-merge    # Gá»™p táº¥t cáº£ CSV files
```

## Performance Tips

### Scaling Options:

- **Safe mode (1 worker)**: Ãt rá»§i ro, cháº­m hÆ¡n
- **Fast mode (2 workers)**: CÃ¢n báº±ng tá»‘c Ä‘á»™/rá»§i ro (khuyáº¿n nghá»‹)

### Memory Management:

- Tá»± Ä‘á»™ng giá»›i háº¡n RAM 3GB/worker
- Garbage collection sau má»—i task
- Worker restart Ä‘á»‹nh ká»³ Ä‘á»ƒ trÃ¡nh memory leak

## Kiáº¿n trÃºc

### Modules chÃ­nh:

- `app/crawler/list_crawler.py`: Crawl danh sÃ¡ch ngÃ nh vÃ  link cÃ´ng ty
- `app/crawler/detail_crawler.py`: Crawl chi tiáº¿t cÃ´ng ty (song song)
- `app/extractor/email_extractor.py`: Extract email báº±ng crawl4ai
- `app/tasks/`: Celery tasks cho xá»­ lÃ½ báº¥t Ä‘á»“ng bá»™
- `app/utils/batching_writer.py`: Ghi CSV an toÃ n theo batch
- `config/`: Há»‡ thá»‘ng config YAML linh hoáº¡t

### Workflow:

1. **Crawl Industries** â†’ Láº¥y danh sÃ¡ch ngÃ nh
2. **Crawl Company Links** â†’ Láº¥y link cÃ´ng ty theo ngÃ nh
3. **Create Tasks** â†’ Chia thÃ nh batch vÃ  gá»­i vÃ o Celery queue
4. **Process Tasks** â†’ Má»—i task crawl chi tiáº¿t + extract email
5. **Merge Results** â†’ Gá»™p táº¥t cáº£ file CSV cuá»‘i cÃ¹ng

## Configuration

### Config cÃ³ sáºµn:

- `default`: Cáº¥u hÃ¬nh máº·c Ä‘á»‹nh
- `1900comvn`: Cáº¥u hÃ¬nh cho 1900.com.vn
- `example`: Cáº¥u hÃ¬nh vÃ­ dá»¥ cho website khÃ¡c

### Táº¡o config má»›i:

```bash
# Copy config cÃ³ sáºµn
cp config/configs/default.yml config/configs/mywebsite.yml

# Chá»‰nh sá»­a file config
vim config/configs/mywebsite.yml

# Validate config
uv run python -m app.main validate --config mywebsite

# Cháº¡y vá»›i config má»›i
uv run python -m app.main crawl --config mywebsite
```

### Cáº¥u trÃºc YAML:

```yaml
website:
  name: "Website Name"
  base_url: "https://example.com"

xpath:
  company_name: "//h1[@class='company-title']"
  company_address: "//div[@class='address']"
  # ... cÃ¡c xpath khÃ¡c

crawl4ai:
  website_query: "Extract business emails from website"
  facebook_query: "Extract business emails from Facebook"

processing:
  batch_size: 30
  write_batch_size: 150
  max_concurrent_pages: 6
  max_retries: 2
  delay_range: [1.5, 3.0]
  timeout: 60000
  network_timeout: 20000
  stealth_mode: true

output:
  output_dir: "data/tasks"
  final_output: "data/companies.csv"

fieldnames:
  - industry_name
  # ... cÃ¡c field khÃ¡c
```

## Dá»¯ liá»‡u Output

### Fields trong CSV:

- `industry_name`: ThÃ´ng tin ngÃ nh
- `name`, `address`, `website`, `phone`: ThÃ´ng tin cÆ¡ báº£n cÃ´ng ty
- `created_year`, `revenue`, `scale`: ThÃ´ng tin kinh doanh
- `link`, `facebook`, `linkedin`, `tiktok`, `youtube`, `instagram`: Social media
- `extracted_emails`: Email Ä‘Æ°á»£c extract (phÃ¢n cÃ¡ch báº±ng "; ")
- `email_source`: Nguá»“n email (Facebook/Website/N/A)

### Phone number format:

- Tá»± Ä‘á»™ng clean vÃ  format thÃ nh dáº¡ng: `+84933802408`
- Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t, dáº¥u cÃ¡ch
- Chuyá»ƒn Ä‘á»•i tá»« 0 thÃ nh +84 (VD: 0933802408 â†’ +84933802408)
- Giá»¯ nguyÃªn náº¿u Ä‘Ã£ cÃ³ +84
- Äáº£m báº£o Ä‘á»™ dÃ i 11-12 sá»‘ (bao gá»“m 84)

## Development

### Setup Development Environment:

```bash
# CÃ i Ä‘áº·t dependencies
source ./.venv/bin/activate.sh
pip install -r requirements.txt

# Hoáº·c vá»›i uv
uv pip install -r requirements.txt
```

### Project Structure:

```
pcrawler/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ crawler/          # Crawling modules
â”‚   â”œâ”€â”€ extractor/        # Email extraction
â”‚   â”œâ”€â”€ tasks/           # Celery tasks
â”‚   â”œâ”€â”€ utils/           # Utilities
â”‚   â””â”€â”€ main.py          # Main orchestrator
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ configs/         # YAML configurations
â”‚   â””â”€â”€ crawler_config.py # Config loader
â”œâ”€â”€ tests/               # Test files
â”œâ”€â”€ data/               # Output data
â”œâ”€â”€ docker-compose.yml  # Docker setup
â”œâ”€â”€ Makefile           # Development commands
â””â”€â”€ pyproject.toml     # Project metadata
```

## Docker

### Services:

- `redis`: Message broker cho Celery
- `worker`: Celery worker xá»­ lÃ½ tasks (cÃ³ thá»ƒ scale)
- `app`: Main application

### Memory Limits:

- **Worker**: 3GB RAM limit, 2GB reservation
- **Max tasks per child**: 20 tasks (tá»± Ä‘á»™ng restart)
- **Max memory per child**: 2GB (tá»± Ä‘á»™ng restart náº¿u vÆ°á»£t)

### Environment Variables:

```bash
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0
CELERY_WORKER_MAX_TASKS_PER_CHILD=20
CELERY_WORKER_MAX_MEMORY_PER_CHILD=2000000
```

## Monitoring & Logging

### Memory Monitoring:

```bash
# Xem logs vá»›i memory info
make docker-logs

# Memory logs sáº½ hiá»ƒn thá»‹:
# [MEMORY][Task xxx] start: 150 MB
# [MEMORY][Task xxx] after GC: 120 MB (freed ~30 MB)
# [MEMORY][Batch] before: 200 MB
# [MEMORY][Batch] after GC: 180 MB (freed ~20 MB)
```

### Log Levels:

- `DEBUG`: Chi tiáº¿t nháº¥t, dÃ nh cho development
- `INFO`: ThÃ´ng tin chung (máº·c Ä‘á»‹nh)
- `WARNING`: Cáº£nh bÃ¡o
- `ERROR`: Lá»—i

### Docker Logs:

```bash
# Xem táº¥t cáº£ logs
make docker-logs

# Xem logs worker
docker-compose logs -f worker

# Xem logs app
docker-compose logs -f app
```

## Error Handling

### Retry Logic:

- Tá»± Ä‘á»™ng retry khi crawl tháº¥t báº¡i
- Configurable retry count vÃ  delay
- Graceful handling cá»§a network errors

### Task Recovery:

- Má»—i task táº¡o file riÃªng
- CÃ³ thá»ƒ gá»™p láº¡i file náº¿u quÃ¡ trÃ¬nh bá»‹ giÃ¡n Ä‘oáº¡n
- KhÃ´ng máº¥t dá»¯ liá»‡u khi task fail

## Security & Best Practices

### Rate Limiting:

- Configurable delay giá»¯a cÃ¡c request
- Respect robots.txt
- User-Agent rotation

### Data Validation:

- Validate config trÆ°á»›c khi cháº¡y
- Clean vÃ  format dá»¯ liá»‡u
- Validate email format

## Contributing

1. Fork project
2. Táº¡o feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Táº¡o Pull Request

### Code Style:

```bash
# Format code (náº¿u cÃ³ black)
uv run black .

# Check linting (náº¿u cÃ³ flake8)
uv run flake8 .

# Run tests (náº¿u cÃ³ pytest)
uv run pytest
```

## License

MIT License - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## Troubleshooting

### Common Issues:

1. **Docker khÃ´ng start:**

   ```bash
   make docker-down
   make docker-build
   make docker-scale-2
   ```

2. **Celery worker khÃ´ng nháº­n tasks:**

   ```bash
   # Check Redis connection
   docker-compose exec redis redis-cli ping

   # Restart worker
   docker-compose restart worker
   ```

3. **Memory usage cao:**

   ```bash
   # Xem memory logs
   make docker-logs | grep MEMORY

   # Worker sáº½ tá»± restart sau 20 tasks hoáº·c 2GB RAM
   # KhÃ´ng cáº§n can thiá»‡p thá»§ cÃ´ng
   ```

4. **Crawl cháº­m:**

   ```bash
   # TÄƒng sá»‘ workers
   make docker-scale-2  # Thay vÃ¬ docker-scale-1

   # Hoáº·c scale cao hÆ¡n (cáº©n tháº­n)
   docker-compose up --scale worker=4 -d
   ```

### Debug Mode:

```bash
# Xem logs chi tiáº¿t
make docker-logs

# Xem logs worker
docker-compose logs -f worker --tail=100

# Xem logs app
docker-compose logs -f app --tail=100
```

## Support

- Email: phuoctv.ut@gmail.com
- Issues: [GitHub Issues](https://github.com/tranvietphuoc/pcrawler/issues)
- Documentation: [Wiki](https://github.com/tranvietphuoc/pcrawler/wiki)
